# AMSDN: Adaptive Multi-Scale Defense Network

A research-grade PyTorch implementation of a unified framework for defending against patch and sparse adversarial attacks.

## ğŸ¯ Overview

AMSDN combines multiple defense mechanisms in a single, end-to-end trainable architecture:

1. **Multi-Scale Feature Extraction** (ConvNeXt-Tiny + FPN)
2. **Adaptive Attention** (Spatial + Channel + Multi-Scale Pyramid)
3. **Selective Purification** (Feature-space denoising)
4. **Prediction Consistency Verification**
5. **Self-Supervised Robustness Training** (SSRT)
6. **Randomized Smoothing Certification**

## ğŸ“Š Architecture

```
Input Image (3Ã—32Ã—32)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: ConvNeXt-Tiny + FPN        â”‚
â”‚ Output: Multi-scale features        â”‚
â”‚ [P2, P3, P4, P5] @ 256 channels     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: Adaptive Attention         â”‚
â”‚ â€¢ Spatial Attention                 â”‚
â”‚ â€¢ Channel Attention                 â”‚
â”‚ â€¢ Multi-Scale Pyramid Attention     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: Selective Purification     â”‚
â”‚ â€¢ Anomaly Detection                 â”‚
â”‚ â€¢ Feature Denoising                 â”‚
â”‚ â€¢ Selective Fusion                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: Prediction Consistency     â”‚
â”‚ (Optional, expensive)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Classification Head                 â”‚
â”‚ Output: Logits (10 classes)         â”‚
â”‚         Anomaly Score               â”‚
â”‚         Detection Decision          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8+
# CUDA-capable GPU (recommended)

pip install -r requirements.txt
```

### Training Pipeline

```bash
# 1. Self-Supervised Pretraining 
python training/pretrain_ssrt.py

# 2. Adversarial Training 
python training/adversarial_train.py

# 3. Multi-Attack Fine-tuning 
python training/finetune_attacks.py

# 4. Evaluation 
python evaluation/evaluate.py

# 5. Certification 
python evaluation/certification.py
```

### Google Colab

Open `notebooks/AMSDN_Colab.ipynb` in Google Colab for a complete interactive tutorial.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/AMSDN/blob/main/notebooks/AMSDN_Colab.ipynb)

## ğŸ“ Repository Structure

```
AMSDN/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cifar10.py                 # CIFAR-10 data loading
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ convnext_fpn.py        # ConvNeXt + FPN
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â””â”€â”€ adaptive_attention.py  # Multi-scale attention
â”‚   â”œâ”€â”€ purification/
â”‚   â”‚   â””â”€â”€ selective_purifier.py  # Adversarial purification
â”‚   â””â”€â”€ amsdn.py                   # Main AMSDN model
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ pretrain_ssrt.py           # Self-supervised pretraining
â”‚   â”œâ”€â”€ adversarial_train.py       # Adversarial training
â”‚   â””â”€â”€ finetune_attacks.py        # Multi-attack fine-tuning
â”‚
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ patch_attacks.py           # Patch attacks (AdvPatch, BPDA)
â”‚   â””â”€â”€ pixel_attacks.py           # Sparse pixel attacks
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate.py                # Comprehensive evaluation
â”‚   â””â”€â”€ certification.py           # Randomized smoothing
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.py                 # Visualization & utilities
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ AMSDN_Colab.ipynb          # Interactive Colab notebook
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”¬ Implemented Attacks

- **PGD** (Projected Gradient Descent): Îµ=8/255, 16/255
- **C&W** (Carlini-Wagner): L2 attack
- **Patch Attacks**: Localized perturbations (4Ã—4, 8Ã—8 pixels)
- **Pixel Attacks**: Sparse perturbations (5, 10 pixels)
- **Adaptive BPDA**: Gradient obfuscation circumvention

## ğŸ™ Acknowledgments

- ConvNeXt architecture from [timm](https://github.com/rwightman/pytorch-image-models)
- Inspired by adversarial defense research from Cohen et al., Brown et al., and others
- Built with PyTorch

## ğŸ“ Related Work

- **Randomized Smoothing:** Cohen et al., "Certified Adversarial Robustness via Randomized Smoothing" (ICML 2019)
- **Adversarial Patch:** Brown et al., "Adversarial Patch" (NIPS 2017 Workshop)
- **PGD Attack:** Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" (ICLR 2018)
- **FPN:** Lin et al., "Feature Pyramid Networks for Object Detection" (CVPR 2017)
- **ConvNeXt:** Liu et al., "A ConvNet for the 2020s" (CVPR 2022)

